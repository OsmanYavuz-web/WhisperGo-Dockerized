# ğŸ³ WhisperGo-Dockerized

Bu proje, OpenAI'nin Whisper modelinin yÃ¼ksek performanslÄ± C/C++ implementasyonu olan [whisper.cpp](https://github.com/ggml-org/whisper.cpp) Ã¼zerine inÅŸa edilmiÅŸ, Dockerize edilmiÅŸ, baÄŸÄ±msÄ±z ve hafif bir API servisidir.

## âœ¨ Ã–zellikler

*   ğŸš€ **YÃ¼ksek Performans:** C/C++ tabanlÄ± motor, Multi-Stage build ile optimize edilmiÅŸ imaj.
*   ğŸ‹ **Docker Ready:** Tek komutla (Docker Compose) ayaÄŸa kalkmaya hazÄ±r.
*   ğŸ”’ **GÃ¼venli:** Non-root (yetkisiz) kullanÄ±cÄ± ile Ã§alÄ±ÅŸarak prodÃ¼ksiyon gÃ¼venliÄŸi saÄŸlar.
*   ğŸŒ **REST API:** Kolay entegrasyon iÃ§in hazÄ±r HTTP server (Queue & Async Support).
*   ğŸ–¥ï¸ **Web ArayÃ¼zÃ¼:** Ses dosyalarÄ±nÄ± tarayÄ±cÄ± Ã¼zerinden test etmek iÃ§in yerleÅŸik Swagger UI.
*   ğŸŒ **Ã‡ok Dilli:** TÃ¼rkÃ§e dahil 99+ dilde yÃ¼ksek doÄŸrulukta transkripsiyon.
*   ğŸ“¦ **Standalone:** Harici hiÃ§bir kÃ¼tÃ¼phaneye veya baÄŸÄ±mlÄ±lÄ±ÄŸa ihtiyaÃ§ duymaz.
*   ğŸ’¾ **CLI Mode:** Her istekte model yÃ¼klenir, bellek sadece iÅŸlem sÄ±rasÄ±nda kullanÄ±lÄ±r.

---

## ğŸš€ Kurulum (SeÃ§iminizi YapÄ±n)

### SeÃ§enek 1: Docker (Ã–nerilen)
HiÃ§bir ÅŸey kurmanÄ±za gerek yok, sadece Docker yeterli.

```bash
# Servisi baÅŸlat (Model yoksa otomatik indirilecektir)
docker-compose up -d --build

# LoglarÄ± takip et
docker-compose logs -f
```
Server: `http://localhost:8080`

---

### SeÃ§enek 2: Local GeliÅŸtirme (Python - Docker'sÄ±z)
EÄŸer kendi makinenizde `whisper.cpp` binary'si ile Ã§alÄ±ÅŸtÄ±rmak isterseniz:

1.  **Gereksinimler:** Python 3.9+, [whisper-cli](https://github.com/ggml-org/whisper.cpp) binary dosyasÄ±.
2.  **Ã‡alÄ±ÅŸtÄ±rma:**

**Windows (PowerShell):**
```powershell
# YollarÄ± kendi sistemine gÃ¶re dÃ¼zenle
$env:WHISPER_CLI_PATH="C:\Tools\whisper.cpp\main.exe"
$env:WHISPER_MODEL_PATH="C:\Tools\whisper.cpp\models\ggml-base.bin"
$env:WHISPER_PORT="8080"

python cli-api.py
```

---

## ğŸ“¡ API KullanÄ±mÄ±

### 1. Swagger UI (Web ArayÃ¼zÃ¼)
TarayÄ±cÄ±dan **[http://localhost:8080/docs](http://localhost:8080/docs)** adresine gidin.

### 2. cURL ile KullanÄ±m

**Senkron (Bekleyerek):**
```bash
curl http://localhost:8080/inference \
  -H "Content-Type: multipart/form-data" \
  -F file="@ses-dosyasi.wav" \
  -F language="tr"
```

**Asenkron (Hemen Job ID Al):**
```bash
curl http://localhost:8080/inference \
  -H "Content-Type: multipart/form-data" \
  -F file="@ses-dosyasi.wav" \
  -F async="true"
```
_DÃ¶nen `job_id` ile durum sorgulama: `/status/{job_id}`_

---

## âš™ï¸ YapÄ±landÄ±rma (`.env`)

VarsayÄ±lan ayarlarÄ± deÄŸiÅŸtirmek iÃ§in `.env` dosyasÄ±nÄ± dÃ¼zenleyin:

| DeÄŸiÅŸken | VarsayÄ±lan | AÃ§Ä±klama |
|----------|------------|----------|
| `PORT` | `8080` | DÄ±ÅŸ eriÅŸim portu |
| `WHISPER_MODEL` | `ggml-base.bin` | Model boyutu (tiny, base, small, medium, large) |
| `WHISPER_TIMEOUT` | `1200` | Ä°ÅŸlem baÅŸÄ±na zaman aÅŸÄ±mÄ± (saniye) |

---

## ğŸ“ KlasÃ¶r YapÄ±sÄ±

```text
WhisperGo-Dockerized/
â”œâ”€â”€ Dockerfile          # Multi-Stage build
â”œâ”€â”€ docker-compose.yml  # Servis orkestrasyonu
â”œâ”€â”€ cli-api.py          # Queue & Thread tabanlÄ± Python API
â”œâ”€â”€ swagger.json        # OpenAPI DokÃ¼mantasyonu
â”œâ”€â”€ .env                # Ayarlar
â””â”€â”€ models/             # Modeller (Otomatik iner)
```

## ğŸ‘¤ HazÄ±rlayan

**HazÄ±rlayan:** Osman Yavuz  
**ğŸ“§ E-posta:** omnyvz.yazilim@gmail.com